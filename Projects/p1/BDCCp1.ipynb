{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDCC project 1 \n",
    "\n",
    "_[Big Data and Cloud Computing](http://www.dcc.fc.up.pt/~edrdo/aulas/bdcc), DCC/FCUP_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code necessary to run from the command line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    # This block is required to run the program from the command line\n",
    "    # in interface with a single Spark instance\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"BDCCp1\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided code - auxilliary functions\n",
    "\n",
    "__You should not need to edit these.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loadMovieLensData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def readCSV(file, debug=False):\n",
    "    if debug:\n",
    "      print('Reading ' + file)\n",
    "    return spark.read.csv(file, inferSchema=True, header=True)\n",
    "\n",
    "def readParquet(file, debug=False): \n",
    "    if debug:\n",
    "       print('Reading ' + file)\n",
    "    return spark.read.parquet(file)\n",
    "\n",
    "def loadMovieLensData(path, format='parquet', debug=False):\n",
    "    if format == 'parquet':\n",
    "       movies = readParquet(path +'/movies.parquet', debug)\n",
    "       ratings = readParquet(path +'/ratings.parquet', debug)\n",
    "       tags = readParquet(path +'/tags.parquet', debug)\n",
    "    else:\n",
    "       movies = readCSV(path +'/movies.csv', debug)\n",
    "       ratings = readCSV(path +'/ratings.csv', debug)\n",
    "       tags = readCSV(path +'/tags.csv', debug)\n",
    "    \n",
    "    tags = tags.withColumn('tagl', F.explode(F.split(F.lower(F.col('tag')),'[ \\*\\+\\&\\/\\%\\-\\$\\#\\'\\)\\(\\[\\[\\],.!?;:\\t\\n\"]+')))\\\n",
    "            .drop('tag')\\\n",
    "            .withColumnRenamed('tagl','tag')\n",
    "    if (debug):\n",
    "        print('> movies')\n",
    "        movies.printSchema()\n",
    "        movies.show()\n",
    "        print('> ratings')\n",
    "        ratings.printSchema()\n",
    "        ratings.show()\n",
    "        print('> tags')\n",
    "        tags.printSchema()\n",
    "        tags.show()\n",
    "    return (movies, ratings, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writeCSV / writeParquet (use them to write a data frame to CSV or Parquet format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(df, path): \n",
    "    df.write.csv(path, header=True, mode='overwrite')\n",
    "\n",
    "def writeParquet(df,path):\n",
    "    df.write.parquet(path, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### createTagListDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTagListDF(csvTagList):\n",
    "    return spark.createDataFrame([ (t,) for t in csvTagList.split(' ')], ['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions available only in Spark 2.4 (GCP Spark instances run Spark 2.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType,IntegerType\n",
    "\n",
    "# Define F.array_intersect if not defined (Spark version < 2.4)\n",
    "if not hasattr(F,'array_intersect'):\n",
    "  F.array_intersect = spark.udf\\\n",
    "    .register('array_intersect', \n",
    "       lambda x,y: list(set(x) & set(y)), ArrayType(IntegerType()))\n",
    "\n",
    "# Define F.array_union if not defined (Spark version < 2.4)\n",
    "if not hasattr(F,'array_union'):\n",
    "  F.array_union = spark.udf\\\n",
    "    .register('array_union', \n",
    "       lambda x,y: list(set(x) | set(y)), ArrayType(IntegerType()))\n",
    "\n",
    "# Define F.array_except if not defined (Spark version < 2.4)\n",
    "if not hasattr(F,'array_except'):\n",
    "  F.array_except = spark.udf\\\n",
    "    .register('array_except', \n",
    "       lambda x,y: list(set(x) - set(y)), ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to define \n",
    "\n",
    "__This is the section that will be evaluated.__\n",
    "\n",
    "__Include your code for the various functions required in the assigment below.__\n",
    "\n",
    "__You may include other auxilliary functions required for computation here\n",
    "but NOT test code (see below).__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidfTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to compute the tfidf of a given DF\n",
    "def tfidf_aux(data, term, document, debug=False):\n",
    "    \"\"\"Data is the Dataframe to apply tf-idf to.\n",
    "    term is the name of the column containing the terms.\n",
    "    document is the name of the column containing the documents\"\"\"\n",
    "    \n",
    "    f = data\\\n",
    "        .groupBy(term, document)\\\n",
    "            .agg(F.count(document)\\\n",
    "                 .alias('f')\\\n",
    "                )\n",
    "    if debug:\n",
    "        print('>>> TF-IDF Debugger')\n",
    "        print('>>> Step 1 :: Compute number of times ' + term +\n",
    "              'has been used in association to ' + document)\n",
    "        f.show()\n",
    "    \n",
    "    f_max = f.groupBy(document)\\\n",
    "                .agg(F.max('f')\\\n",
    "                     .alias('f_max')\\\n",
    "                )\n",
    "    f_f_max = f.join(f_max, document)\n",
    "    if debug:\n",
    "        print('>>> Step 2 :: Compute maximum absolute frequence of any ' + term +\n",
    "              ' used for ' + document)\n",
    "        f_f_max.show()\n",
    "    \n",
    "    tf = f_f_max\\\n",
    "            .withColumn('TF', f_f_max.f / f_f_max.f_max)\n",
    "    if debug:\n",
    "        print('>>> Step 3 :: TF value of ' + term + ' for ' + document)\n",
    "        tf.show()\n",
    "    \n",
    "    n = data\\\n",
    "        .groupBy(term)\\\n",
    "        .agg(F.countDistinct(document)\\\n",
    "             .alias('n')\\\n",
    "        )\n",
    "    tf_n = tf.join(n, term)\n",
    "    if debug:\n",
    "        print('>>> Step 4 :: Join with the number of ' + document +\n",
    "              's with ' + term + ' at least once')\n",
    "        tf_n.show()\n",
    "\n",
    "    N = tags.select(document).distinct().count()\n",
    "    idf = tf_n\\\n",
    "            .withColumn('IDF',  F.log2(N / tf_n.n))\n",
    "    if debug:\n",
    "        print('>>> Step 5 :: IDF value of ' + term +\n",
    "              ' considering all ' + document + 's with ' + term)\n",
    "        idf.show()\n",
    "    \n",
    "    tfidf = idf\\\n",
    "                .withColumn('TF_IDF',idf.TF * idf.IDF)\n",
    "    if debug:\n",
    "        print('>>> Step 6 :: TF-IDF value of ' + term + ' for ' + document)\n",
    "        tfidf.show()\n",
    "        print('>>> Finished TF-IDF processing')\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def tfidfTags(tags, debug=False):\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Compute tfidf using \"tag\" as term and \"movieId\" as document')\n",
    "\n",
    "    return tfidf_aux(tags, 'tag', 'movieId', debug)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recommendByTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def recommendByTag(singleTag, TFIDF_tags, movies, min_fmax=10, numberOfResults=10, debug=False):\n",
    "    filt_tags = TFIDF_tags\\\n",
    "                        .filter((TFIDF_tags.tag == singleTag) &\\\n",
    "                                (TFIDF_tags.f_max >= min_fmax))\\\n",
    "                        .drop('tag', 'f', 'f_max', 'n', 'TF', 'IDF')\n",
    "    if debug:\n",
    "        print('>> Step 1 :: TFIDF of single tag & Filtered by >= ' + str(min_fmax))\n",
    "        filt_tags.show()\n",
    "\n",
    "    tags_movie = filt_tags.join(movies, 'movieId')\n",
    "    if debug:\n",
    "        print('>> Step 2 :: Join with the corresponding movie')\n",
    "        tags_movie.show()\n",
    "\n",
    "    rm_tag = tags_movie\\\n",
    "                .orderBy(['TF_IDF', 'title'], ascending=[0, 1])\\\n",
    "                .select('movieId', 'title', 'TF_IDF')\\\n",
    "                .limit(numberOfResults)\n",
    "    if debug:\n",
    "        print('>> Step 3 :: Limit to ' + str(numberOfResults) + ' ordered results')\n",
    "        rm_tag.show()\n",
    "\n",
    "    return rm_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recommendByTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Can it be done by using previous function recommendByTag?\n",
    "# Even if possible is more computationally heavy\n",
    "def recommendByTags(searchTags, TFIDF_tags, movies, min_fmax=10, numberOfResults=10, debug=False):\n",
    "    searchTagsDF = createTagListDF(searchTags)\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Search tags DF: ' + searchTags)\n",
    "        searchTagsDF.show()\n",
    "\n",
    "    filt_tags = TFIDF_tags\\\n",
    "                    .join(searchTagsDF, 'tag')\\\n",
    "                    .filter(F.col('f_max') >= min_fmax)\n",
    "    if debug:\n",
    "        print('>> Step 2 :: TFIDF of given tags & filtered by >= ' + str(min_fmax))\n",
    "        filt_tags.show()\n",
    "\n",
    "    sum_tfidf = filt_tags\\\n",
    "                    .groupBy('movieID')\\\n",
    "                    .agg(F.sum('TF_IDF')\\\n",
    "                        .alias('SUM_TF_IDF')\\\n",
    "                    )\n",
    "    if debug:\n",
    "        print('>> Step 3 :: Sum of TF_IDF on same movies')\n",
    "        sum_tfidf.show()\n",
    "\n",
    "    tags_movie = sum_tfidf\\\n",
    "                    .join(movies, 'movieId')\\\n",
    "                    .orderBy(['SUM_TF_IDF', 'title'], ascending=[0, 1])\\\n",
    "                    .select('movieId', 'title', 'SUM_TF_IDF')\\\n",
    "                    .limit(numberOfResults)\n",
    "    if debug:\n",
    "        print('>> Step 4 :: Join with the corresponding movie & limit to ' +\n",
    "              str(numberOfResults) + ' ordered results')\n",
    "        tags_movie.show()\n",
    "\n",
    "    return tags_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jiMovieSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jiSimilarity(data, col_ref, col_set, min_col_set=0, debug=False):\n",
    "    \"\"\"\"Data is the Dataframe to apply Jaccard Index to.\n",
    "    col_ref is the name of the column of reference for the sets.\n",
    "    col_ref should have as a last character a '1'.\n",
    "    col_set is the name of the column to generate the sets from.\n",
    "    min_col_set is the minimum size of the admitted sets\"\"\"\n",
    "\n",
    "    f1 = data\\\n",
    "                .groupBy(col_ref)\\\n",
    "                .agg(F.collect_set(data[col_set])\\\n",
    "                      .alias('f1')\\\n",
    "                    )\\\n",
    "                .filter(F.size('f1') >= min_col_set)\n",
    "    if debug:\n",
    "        print('>>> Jaccard Index debugger')\n",
    "        print('>>> Step 1 :: ' + col_ref + ' & Set of ' + col_set + \\\n",
    "                  ' that are related with' + col_ref + ' (f1) & Have sets size ' +\n",
    "                  'bigger than ' + str(min_col_set))\n",
    "        f1.show()\n",
    "\n",
    "    col_ref_2 = col_ref[:-1] + '2'\n",
    "    f2 = f1\\\n",
    "            .withColumnRenamed(col_ref, col_ref_2)\\\n",
    "            .withColumnRenamed('f1', 'f2')\n",
    "\n",
    "    cross_prod = f1\\\n",
    "                .crossJoin(f2)\\\n",
    "                .filter(f1[col_ref] < f2[col_ref_2])\n",
    "    if debug:\n",
    "        print('>>> Step 2 :: Crossing different ' + col_ref[:-1] +\n",
    "              ' and the respective sets of ' + col_set)\n",
    "        cross_prod.show()\n",
    "\n",
    "    i_u = cross_prod\\\n",
    "                .withColumn('i', F.size(\\\n",
    "                             F.array_intersect(cross_prod.f1,\\\n",
    "                                               cross_prod.f2)\\\n",
    "                                       )\\\n",
    "                           )\\\n",
    "                .withColumn('u', F.size(\\\n",
    "                           F.array_union(cross_prod.f1,\\\n",
    "                                         cross_prod.f2)\\\n",
    "                                       )\\\n",
    "                           )\\\n",
    "                .drop('f1', 'f2')\n",
    "    if debug:\n",
    "        print('>>> Step 3 :: Intersection between ' + col_set +\n",
    "              ' (i) & Union between ' + col_set + ' (u)')\n",
    "        i_u.show()\n",
    "\n",
    "    ji = i_u\\\n",
    "            .withColumn('JI', i_u.i / i_u.u)\n",
    "    if debug:\n",
    "        print('>>> Step 4 :: Computed JI out of i & u')\n",
    "        ji.show()\n",
    "        print('>>> Finished Jaccard Index processing')\n",
    "\n",
    "    return ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def jiMovieSimilarity(ratings, minRatings=10, debug=False):\n",
    "    liked_ratings = ratings\\\n",
    "                        .filter(ratings.rating >= 4.0)\\\n",
    "                        .withColumnRenamed('movieId', 'm1')\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Filter ratings for liked movies & rename movieId to m1')\n",
    "        liked_ratings.show()\n",
    "        print('>> Step 2 :: Compute JI using \"m1\" as col_ref and \"userId\" as col_set')\n",
    "\n",
    "    return jiSimilarity(liked_ratings, 'm1', 'userId', minRatings, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recommendBySimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJiEntries(ji, entry_id, col_name_1, debug=False):\n",
    "    \"\"\"Gets the Entries of a Jaccard Index for the first two columns.\n",
    "    Since in the given JI col1 < col2 we want to retrieve the DF were entry_id can be either\n",
    "    on col1 or col2. \n",
    "    col_name_1 is the name of the first column in the JI.\n",
    "    The col1 or col2 results will be returned in a column named\n",
    "    col_name_1[:-1] (removing the digit of the given column name)\"\"\"\n",
    "\n",
    "    col_name = col_name_1[:-1]\n",
    "    col_name_2 = col_name + '2'\n",
    "    \n",
    "    col1_ji = ji\\\n",
    "                .filter(ji[col_name_1] == entry_id)\\\n",
    "                .drop(col_name_1, 'i', 'u')\n",
    "    if debug:\n",
    "        print('>>> GetJiEntries Debugger')\n",
    "        print('>>> Step 1 :: Filter ji where col1 is ' + str(entry_id))\n",
    "        col1_ji.show()\n",
    "\n",
    "    col2_ji = ji\\\n",
    "                .filter(ji[col_name_2] == entry_id)\\\n",
    "                .drop(col_name_2, 'i', 'u')\n",
    "    if debug:\n",
    "        print('>>> Step 2 :: Filter ji where col2 is ' + str(entry_id))\n",
    "        col2_ji.show()\n",
    "\n",
    "    col_ji = col1_ji\\\n",
    "                .withColumnRenamed(col_name_2, col_name)\\\n",
    "                .union(\\\n",
    "                       col2_ji\\\n",
    "                           .withColumnRenamed(col_name_1, col_name)\\\n",
    "                      )\n",
    "    if debug:\n",
    "        print('>>> Step 3 :: Union of the two DFs presented before')\n",
    "        col_ji.show()\n",
    "        print('>>> Finished GetJiEntries processing')\n",
    "\n",
    "    return col_ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendBySimilarity(movieId, movies, jiForMovies, numberOfResults=10, debug=False):\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Get JI entries for the given Jaccard Index')\n",
    "\n",
    "    ji_movieId = getJiEntries(jiForMovies, movieId, 'm1', debug)\\\n",
    "                    .withColumnRenamed('m', 'movieId')\n",
    "\n",
    "    result = ji_movieId\\\n",
    "                    .join(movies, 'movieId')\\\n",
    "                    .select('movieId', 'title', 'JI')\\\n",
    "                    .orderBy('JI', ascending=False)\\\n",
    "                    .limit(numberOfResults)\n",
    "    if debug:\n",
    "        print('>> Step 2 :: Join with the respective movies and order results')\n",
    "        result.show()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify input data set and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "bucket = 'gs://bdcc_up201503784_311' # Ed's bucket \n",
    "#bucket = 'gs://bdcc_up201503316' # Foo's bucket \n",
    "path = '/p1/data/'\n",
    "#path = '/p1/'\n",
    "dataset = 'medium2'\n",
    "fullPath = bucket + path + dataset\n",
    "\n",
    "(movies, ratings, tags) = \\\n",
    "  loadMovieLensData(fullPath, format='csv', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test code \n",
    "\n",
    "__Include test code below that you may need here.__\n",
    "\n",
    "__The initial contents are only meant as an example.__\n",
    "\n",
    "__This section will NOT be evaluated.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TF-IDF for tags\n",
    "tfidf = tfidfTags(tags)\n",
    "\n",
    "# tfidf.cache()\n",
    "# tfidf.orderBy(['movieId','TF_IDF'],ascending=[1,0]).show()\n",
    "tfidf.orderBy(['f','TF_IDF','movieId','tag'],ascending=[0,0,1,1]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend by tag \n",
    "\n",
    "rm = recommendByTag('cartoon', tfidf, movies)\n",
    "rm.show()\n",
    "\n",
    "rm = recommendByTag('cartoon', tfidf, movies, min_fmax=1)\n",
    "rm.show()\n",
    "\n",
    "\n",
    "rm = recommendByTag('cruise', tfidf, movies)\n",
    "rm.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend by Tags\n",
    "\n",
    "rm = recommendByTags('hitchcock birds', tfidf, movies)\n",
    "rm.show()\n",
    "\n",
    "rm = recommendByTags('quentin tarantino', tfidf, movies)\n",
    "rm.show()\n",
    "\n",
    "rm = recommendByTags('sci fi space', tfidf, movies)\n",
    "rm.show()\n",
    "\n",
    "#rm = recommendByTags('hitchcock birds', tfidf, movies, numberOfResults=10)\n",
    "#rm.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jiM = jiMovieSimilarity(ratings)\n",
    "\n",
    "#jiM.orderBy(['JI','m1','m2'], ascending=[0,1,1]).show()\n",
    "jiM.orderBy(['i','JI','m1','m2'], ascending=[0,0,1,1]).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jiM.cache()\n",
    "\n",
    "# The Dish\n",
    "sm = recommendBySimilarity(4225, movies, jiM)\n",
    "sm.show()\n",
    "\n",
    "# Miami Vice\n",
    "sm = recommendBySimilarity(47044, movies, jiM)\n",
    "sm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidfMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfMoviesAndTags(movies, tags, debug=False):\n",
    "    movie_title_w = movies\\\n",
    "                .withColumn('word',\\\n",
    "                    F.explode(F.split(F.col('title'), '( \\([0-9]{4}\\))| '))\\\n",
    "                           )\\\n",
    "                .filter(F.col('word') != '')\\\n",
    "                .drop('title')\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Associate to each movie the words belonging to its title')\n",
    "        movie_title_w.show()\n",
    "\n",
    "    # Union keeps duplicates - intended\n",
    "    movie_w = movie_title_w\\\n",
    "                    .union(tags\\\n",
    "                              .withColumnRenamed('tag', 'word')\\\n",
    "                              .drop('userId'))\n",
    "    if debug:\n",
    "        print('>> Step 2 :: Union of previous DF with the given tags')\n",
    "        movie_w.orderBy('movieId').show()\n",
    "        print('>> Step 3 :: Compute tfidf using \"word\" as term and \"movieId\" as document')\n",
    "\n",
    "    return tfidf_aux(movie_w, 'word', 'movieId', debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jiTagSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jiTagSimilarity(tags, debug=False):\n",
    "    t1_tags = tags\\\n",
    "                .withColumnRenamed('tag', 't1')\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Rename tag to t1')\n",
    "        t1_tags.show()\n",
    "        print('>> Step 2 :: Compute JI using \"t1\" as col_ref and \"movieId\" as col_set')\n",
    "\n",
    "    return jiSimilarity(t1_tags, 't1', 'movieId', 0, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommendTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def recommendTags(movieId, jiTags, tags, numberSimilarTags=5, debug=False):\n",
    "    movie_tags = tags\\\n",
    "                    .filter(tags.movieId == movieId)\n",
    "    movie_tags_list = [row.tag for row\\\n",
    "                               in movie_tags.distinct().collect()]\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Get the tags associated to the given movieId')\n",
    "        movie_tags.show()\n",
    "        print('>> Step 1 :: Representing the tags in a list: ')\n",
    "        print(movie_tags_list)\n",
    "\n",
    "    df_array = [getJiEntries(jiTags, movie_tag, 't1')\\\n",
    "                            .filter(~F.col('t').isin(movie_tags_list))\\\n",
    "                            .orderBy('JI', ascending=False)\n",
    "                    for movie_tag\n",
    "                    in movie_tags_list]\n",
    "    if debug:\n",
    "        print('>> Step 2 :: For each movie tag, get the tags not yet associated' +\n",
    "                'with movieId that maximize the JI')\n",
    "        for df in df_array:\n",
    "            df.show()\n",
    "\n",
    "    res_df = reduce(\\\n",
    "                lambda acc, df: acc.union(df),\\\n",
    "                df_array)\n",
    "    if debug:\n",
    "        print('>> Step 3 :: Union of previous DFs')\n",
    "        res_df.show()\n",
    "\n",
    "    result = res_df\\\n",
    "                .groupBy('t')\\\n",
    "                .agg(F.sum('JI')\\\n",
    "                    .alias('SUM_JI')\\\n",
    "                )\\\n",
    "                .orderBy('SUM_JI', ascending=False)\\\n",
    "                .limit(numberSimilarTags)\n",
    "    if debug:\n",
    "        print('>> Step 4 :: Summed all the JIs referencing the given tag and ' +\n",
    "                'limited to ' + str(numberSimilarTags) + ' ordered results. ' +\n",
    "                'The higher the JI sum, the more relevant is the tag')\n",
    "        result.show()\n",
    "\n",
    "    return result.drop('SUM_JI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jiUserSimiliarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Jaccard similarity between users based\n",
    "# on what films they rate (independently of the value of the rating itself). \n",
    "def jiUserSimilarity(ratings, debug=False):\n",
    "    u1_rat = ratings\\\n",
    "                .withColumnRenamed('userId', 'u1')\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Rename userId to u1')\n",
    "        u1_rat.show()\n",
    "        print('>> Step 2 :: Compute JI using \"u1\" as col_ref and \"movieId\" as col_set')\n",
    "\n",
    "    return jiSimilarity(u1_rat, 'u1', 'movieId', 0, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommendByUserSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, lit, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Given an array returns a new array with the lit function applied to every member\n",
    "def get_lit_array_from(arr):\n",
    "    ret = [ lit(item) for item in arr[0] ]\n",
    "    return array(ret)\n",
    "\n",
    "# Given an array of movies and an array of ratings \n",
    "# returns the top rated movie\n",
    "# Assumes index relation between arrays \n",
    "# ratings[0] rates movie[0]\n",
    "def get_top_rated(movies, ratings):\n",
    "    m = -1\n",
    "    tr = -1\n",
    "    for i in range(0, len(movies)):\n",
    "        if ratings[i] > m:\n",
    "            m = ratings[i]\n",
    "            tr = movies[i]\n",
    "    return tr\n",
    "\n",
    "# Given an array of movies, an array of ratings and an array of movies to exclude\n",
    "# returns the rating of the movies included\n",
    "def get_ratings(movies, ratings, except_movies):\n",
    "    temp_ratings = []\n",
    "    for i in range(0, len(movies)):\n",
    "        if movies[i] not in except_movies:\n",
    "            temp_ratings.append(ratings[i])\n",
    "    return temp_ratings\n",
    "\n",
    "# Given the id of a user, recommend the top-rated film per each of the most n\n",
    "# similar users to user u, as long as u has not yet rated or tagged the movies at stake.\n",
    "def recommendByUserSimilarity(userID, ratings, tags, jiForUsers, numberSimilarUsers=10, debug=False):\n",
    "    related_users = getJiEntries(jiForUsers, userID, 'u1')\n",
    "    if debug:\n",
    "        print('>> Step 1 :: Get the Jaccard Index of related users ' \\\n",
    "              'to User' + str(userID))\n",
    "        related_users.show()\n",
    "     \n",
    "    relatedUsers_movies = related_users\\\n",
    "            .join(ratings, ratings.userId == related_users.u)\\\n",
    "            .groupBy('u')\\\n",
    "            .agg(F.collect_list('movieId')\\\n",
    "                 .alias('movies'),\\\n",
    "                 F.collect_list('rating')\\\n",
    "                .alias('ratings'))\n",
    "    \n",
    "    if debug:\n",
    "        print('>> Step 2 :: Join of the DFs with ratings')\n",
    "        relatedUsers_movies.show()\n",
    "\n",
    "    u1_mu1 = ratings.union(tags)\\\n",
    "                .filter(F.col('userId') == userID)\\\n",
    "                .drop('rating') \\\n",
    "                .agg(F.collect_set(F.col('movieId'))\\\n",
    "                      .alias('movies'))\n",
    "    \n",
    "    if debug:\n",
    "        print('>> Step 3 :: Set of movies rated/tagged by User' + str(userID))\n",
    "        u1_mu1.show()\n",
    "        \n",
    "    arr = get_lit_array_from(u1_mu1.collect()[0])\n",
    "    relatedUsers_exceptMovies = relatedUsers_movies \\\n",
    "                            .withColumn(\"except_movies\", arr)\n",
    "    \n",
    "    \n",
    "    potencialMovies = F.array_except( \\\n",
    "                    relatedUsers_exceptMovies.movies \\\n",
    "                    ,relatedUsers_exceptMovies.except_movies \\\n",
    "                    )\n",
    "    potencialRatings = udf(get_ratings, ArrayType(FloatType()))\n",
    "    \n",
    "    relatedUsers_potencialMovies=relatedUsers_exceptMovies \\\n",
    "                                .withColumn(\"potencialMovies\", potencialMovies) \\\n",
    "                                .filter(F.size(F.col('potencialMovies')) > 0) \\\n",
    "                                .withColumn(\"potencialRatings\",\\\n",
    "                                    potencialRatings(\"potencialMovies\",\\\n",
    "                                                \"ratings\", \"except_movies\")) \\\n",
    "                                .drop('movies', 'except_movies', 'ratings') \\\n",
    "                                .join(related_users, ['u']) \\\n",
    "                                .orderBy('JI', ascending=False) \\\n",
    "                                .drop('JI') \\\n",
    "                                .limit(numberSimilarUsers)\n",
    "\n",
    "    if debug:\n",
    "        print('>> Step 4 :: Potencial Movies Only')\n",
    "        relatedUsers_potencialMovies.show()\n",
    "\n",
    "        \n",
    "    top_rated = udf(get_top_rated)\n",
    "    \n",
    "    result = relatedUsers_potencialMovies \\\n",
    "            .withColumn(\"topRated\", top_rated(\"potencialMovies\", \"potencialRatings\"))\\\n",
    "            .drop('potencialMovies', 'ratings', 'potencialRatings')\n",
    "    \n",
    "\n",
    "    if debug:\n",
    "        print('>> Step 5 :: Choose the top rated movies')\n",
    "        result.show()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code for Extended Fuctionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tests for tfidfMovies\n",
    "\n",
    "tfidfMT = tfidfMoviesAndTags(movies, tags, debug=True)\n",
    "\n",
    "tfidfMT.orderBy(['f','TF_IDF','movieId','word'],ascending=[0,0,1,1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tests for jiTagSimilarity\n",
    "jiT = jiTagSimilarity(tags)\n",
    "jiT.cache()\n",
    "jiT.orderBy('JI', ascending=False).show()\n",
    "\n",
    "# MovieId 2\n",
    "recommendTags(2, jiT, tags, debug=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tests for jiUserSimilarity\n",
    "jiU = jiUserSimilarity(ratings, debug=True)\n",
    "\n",
    "#jiU.orderBy(['JI','m1','m2'], ascending=[0,1,1]).show()\n",
    "#jiU.orderBy(['i','JI','m1','m2'], ascending=[0,0,1,1]).show()\n",
    "\n",
    "jiU.cache()\n",
    "\n",
    "# User 62\n",
    "sm = recommendByUserSimilarity(62, ratings, tags, jiU, 10, True)\n",
    "sm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}